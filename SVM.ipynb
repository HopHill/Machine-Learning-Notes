{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 支持向量机算法知识点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 支持向量机算法的由来\n",
    "\n",
    "支持向量机于1995年正式发表，由苏联统计学家弗拉基米尔·瓦普尼克发表于machine learning期刊[1],由于在文本分类任务中显示出卓越性能[2]，很快成为机器学习的主流技术，并直接掀起了“统计学习”在2000年前后的高潮。但实际上，支持向量的概念在二十世纪六十年代就已经出现，统计学习理论在七十年代就已成型。对核函数的研究更早，Mercer定理可追溯到1909年，RKHS则在四十年代就已被研究，但在统计学习兴起后，核技巧才真正成为机器学习的通用基本技术。\n",
    "\n",
    "references：\n",
    "\n",
    "[1]Cortes,C. and V.N.Vapnik.(1995).\"Support vector networks\".Machine Learning,20(3):273-297\n",
    "\n",
    "[2]Joachims,T.(1998).\"Text classification with support vector machines:Learning with many relevant features.\" In Proceedings of the 10th European Conference on Machine Learning(ECML),137-142,Chemnitz,Germany.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 支持向量机的作用\n",
    "\n",
    "SVM是一个功能强大并且全面的机器学习模型，它能够执行线性或非线性分类、回归，甚至是异常值检测任务。SVM特别适用于中小型复杂数据集的分类[3]。\n",
    "\n",
    "references:\n",
    "\n",
    "[3]机器学习实战-基于Scikit-Learn 和 TensorFlow.Aurelien Geron[著].王静源等人[译].机械工业出版社.2018(1):p136\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 支持向量机的基本思想\n",
    "\n",
    "支持向量机的基本思想是拟合类别之间可能的、最宽的“街道”（决策边界）。换言之，它的目的是使得决策边界之间的间隔最大化，从而分隔出两个类别的训练实例。SVM执行软间隔分类时，实际上是在完美分类和拟合最宽的街道之间进行妥协（也就是允许少数实例最终落在街道上）。还有一个关键点是在训练非线性数据集时，记得使用核函数。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 什么是支持向量（support vector）\n",
    "\n",
    "支持向量是指SVM模型训练完成后，位于“街道”之上的实例被称为支持向量，这也包括位于边界上的实例。决策边界完全由支持向量决定。非支持向量的实例（也就是街道之外的实例）完全没有任何影响，你可以选择删除他们然后添加更多的实例，或者将他们移开，只要一直在街道之外，他们就不会对决策边界产生任何影响。计算预测结果只会涉及支持向量，而不涉及整个数据集。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 支持向量机的优点\n",
    "\n",
    "SVM既可以用于分类任务（SVC），也用于回归预测任务（SVR），是一个功能强大并且全面的模型，而且能够执行线性或非线性分类、回归，甚至是异常检测任务。\n",
    "支持向量机在中小型复杂数据集上的分类效果较好\n",
    "SVM是一种有坚实理论基础的新颖的小样本学习方法，他基本上不涉及概率测度及大数定律等，因此不同于现有的统计方法，从本质上看，它避免了从归纳到演绎的传统过程，实现了高效的从训练样本到预报样本的“转导推理”，大大简化了通常的分类和回归等问题\n",
    "SVM的最终决策函数只由少数的支持向量所确定，计算的复杂性取决于支持向量的数目，而不是样本空间的维数，这在某种意义上避免了“维度灾难”\n",
    "少数支持向量决定了最终成果，这不但可以帮助我们抓住关键样本、“剔除”大量冗余样本，而且注定了该方法不但算法简单，而且具有较好的“鲁棒性”：主要体现在：第一，增删非支持向量对模型没有影响；第二，支持向量样本集具有一定的鲁棒性；第三，有些成功的应用中，SVM方法对核（Kernel）的选取不敏感\n",
    "\n",
    "Reference:\n",
    "https://blog.csdn.net/qq_38734403/article/details/80442535\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 支持向量机的缺点\n",
    "\n",
    "SVM对特征缩放十分敏感，需要在训练之前对特征进行标准化\n",
    "SVM算法对大规模训练样本难以实施：由于SVM是借助二次规划来解支持向量，而求解二次规划将涉及m阶矩阵的计算（m表示样本数），当m很大时该矩阵的存储和计算开销较大。\n",
    "用SVM解决多分类问题存在困难：经典的支持向量机算法只给出了二分类任务，可以通过OVO，OVR模式和SVM决策树解决\n",
    "\n",
    "Reference：\n",
    "https://blog.csdn.net/qq_38734403/article/details/80442535\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn中的支持向量机模块\n",
    "\n",
    "在sklearn中，有SVC和SVR，SVC用于分类，SVR用于回归，有线性SVC（LinearSVC）、非线性SVC（PolynomialFeatures）及SVC类*（SVC（kernel=’linear’,c=1））。对于线性可分类问题，建议使用LinearSVC，计算速度较快，不推荐使用SVC类。需要注意的是，在使用SVC模型前，需要对特征进行缩放，使用sklearn.preprocessing.StandardScaler（）函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X=iris['data'][:,(2,3)]\n",
    "y=(iris['target']==2).astype(np.float64)#iris -Virginina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('linear_svc', LinearSVC(C=1, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',\n",
       "     penalty='l2', random_state=None, tol=0.0001, verbose=0))])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf = Pipeline((\n",
    "(\"scaler\",StandardScaler()),\n",
    "(\"linear_svc\",LinearSVC(C=1,loss=\"hinge\")),))\n",
    "svm_clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf.predict([[5.5,1.7]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 与logistic回归分类器不同，SVM分类器不会输出每个类别的概率。\n",
    "\n",
    "上面的代码中，也可以使用SVC类，使用SVC('kernel'=\"linear\",C=1),但是这要慢的多，特别是对于大型训练集而言，因此不推荐使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('linear_svc', SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm_clf = Pipeline((\n",
    "(\"scaler\",StandardScaler()),\n",
    "(\"linear_svc\",SVC(kernel=\"linear\",C=1)),))\n",
    "svm_clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf.predict([[5.5,1.8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
#reference:https://sklearn.apachecn.org/docs/0.21.3/5.html
